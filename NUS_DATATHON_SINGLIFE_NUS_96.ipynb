{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "\n",
    "filepath = \"./data/catB_train.parquet\"\n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(filepath)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_stats = df.describe()\n",
    "modes = numerical_stats.mode().iloc[0]\n",
    "numerical_stats.loc[\"mode\"] = modes\n",
    "numerical_stats.loc[\"mode_freq\"] = modes / len(df) * 100\n",
    "numerical_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "# 17992 rows by 304 columns\n",
    "# 90 numerical\n",
    "# 214 categorical\n",
    "# target variable is `f_purchase_lh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(exclude=\"number\").columns\n",
    "numerical_features = df[df.columns.difference([\"f_purchase_lh\"])].select_dtypes(include=\"number\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"categorical_features:\", categorical_features)\n",
    "\n",
    "# n_months_last_bought is ordinal\n",
    "# race_desc is nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"min_occ_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"n_months_last_bought_lh_e22a6a\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"numerical_features:\", numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numerical_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"f_purchase_lh\"]\n",
    "# 7100 1s, 10892 NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot histograms for numerical columns\n",
    "df.hist(figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find columns with missing values\n",
    "count_missing_per_col = df.isna().sum()\n",
    "percent_missing_per_col = df.isna().mean() * 100\n",
    "cols_with_nan = pd.DataFrame({'count': count_missing_per_col, 'percentage': percent_missing_per_col})\n",
    "cols_with_nan = cols_with_nan[cols_with_nan['count'] > 0].sort_values(by='count', ascending=False)\n",
    "print(\"====Columns with missing values:====\")\n",
    "print(\"Cols_with_nan shape:\", cols_with_nan.shape)\n",
    "cols_with_nan  # drop cols with over 90% nan, fillna the rest with median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Rows with most NaN values:\")\n",
    "\n",
    "# Count the number of NaN values in each row\n",
    "nan_count_per_row = df.isna().sum(axis=1)\n",
    "\n",
    "df_check_nan = df.copy()\n",
    "\n",
    "# Add a new column to the DataFrame with the count of NaN values per row\n",
    "df_check_nan['nan_count'] = nan_count_per_row\n",
    "\n",
    "# Calculate the percentage of NaN values\n",
    "df_check_nan['nan_percent'] = (nan_count_per_row / len(df.columns)) * 100\n",
    "\n",
    "# Create a DataFrame with count and percent of missing values\n",
    "rows_w_missing_data = pd.DataFrame({\n",
    "    'count': nan_count_per_row,\n",
    "    'percent': df_check_nan['nan_percent']\n",
    "})\n",
    "\n",
    "missing_data_sorted = rows_w_missing_data.sort_values(by=['count'], ascending=False)\n",
    "missing_data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "corr_matrix = df.select_dtypes(include=np.number).corr()\n",
    "np.fill_diagonal(corr_matrix.values, np.nan)\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='Blues', fmt='g')\n",
    "np.where(corr_matrix >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop columns with more than 90% nan values\n",
    "cols_to_drop = cols_with_nan[cols_with_nan['percentage'] > 90].index\n",
    "target = df[\"f_purchase_lh\"]\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# remove nan from target and drop target from df\n",
    "target = target.fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill numerical NaN with median\n",
    "numerical_columns = df.select_dtypes(include='number').columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].median())\n",
    "\n",
    "# Fill categorical NaN with mode\n",
    "categorical_columns = df.select_dtypes(exclude='number').columns\n",
    "df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop useless column\n",
    "df = df.drop(columns=[\"clntnum\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop columns with only 1 unique value\n",
    "cols_to_drop = []\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        cols_to_drop.append(col)\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop columns with high correlation (unclear if we want to do this)\n",
    "# corr_matrix = df.select_dtypes(include=np.number).corr()\n",
    "# np.fill_diagonal(corr_matrix.values, np.nan)\n",
    "# corr_matrix = corr_matrix.abs()\n",
    "# corr_matrix = corr_matrix[corr_matrix > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scale numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop other useless columns based on prior knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode nominal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode ordinal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imblearn SMOTE on target due to 10x imbalance"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nominal_data = ['race_desc', 'ctrycode_desc', 'clttype', 'stat_flag', 'min_occ_date',\n",
    "                'cltdob_fix', 'cltsex_fix']"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# cast all other categorical columsn to numerical\n",
    "\n",
    "categorical_columns = df.select_dtypes(include='object').columns\n",
    "columns_to_convert = [col for col in categorical_columns if col not in nominal_data]\n",
    "\n",
    "# Convert categorical columns to numerical\n",
    "df[columns_to_convert] = df[columns_to_convert].astype('category').apply(lambda x: x.cat.codes)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "# imblearn SMOTE\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, n_splits=3, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        self.categorical_features = ['race_desc', 'ctrycode_desc', 'clttype', 'stat_flag', 'min_occ_date',\n",
    "                        'cltdob_fix', 'cltsex_fix']\n",
    "        self.cat_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        self.numerical_features = df.select_dtypes(include=\"number\").columns\n",
    "\n",
    "        self.numeric_transformer = Pipeline(\n",
    "            steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                   (\"scaler\", StandardScaler()),\n",
    "                   ]\n",
    "        )\n",
    "\n",
    "        self.cat_transformer = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"cat\", self.cat_transformer, self.categorical_features),\n",
    "                (\"num\", self.numeric_transformer, self.numerical_features),\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    \n",
    "        self.model = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocessor\", self.preprocessor),\n",
    "                (\"regressor\", RandomForestClassifier(n_estimators=10, max_depth=10, random_state=2109))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def train(self, X, y, max_depth=10, n_estimators=10):\n",
    "        self.model.fit(X, y.ravel())\n",
    "\n",
    "        joblib.dump(self.model, f'random_forest_model_fold_{len(self.models)}.joblib')\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        result = self.model.predict(test_X)\n",
    "        return result\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "X = df\n",
    "y = target.values.reshape(-1, 1)\n",
    "model.train(X, y)\n",
    "res = model.predict(X)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(df):\n",
    "    X_train, X_test = df.iloc[train_index], df.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    model.predict(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    df = hidden_data\n",
    "    \n",
    "    # Find columns with missing values\n",
    "    count_missing_per_col = df.isna().sum()\n",
    "    percent_missing_per_col = df.isna().mean() * 100\n",
    "    cols_with_nan = pd.DataFrame({'count': count_missing_per_col, 'percentage': percent_missing_per_col})\n",
    "    cols_with_nan = cols_with_nan[cols_with_nan['count'] > 0].sort_values(by='count', ascending=False)\n",
    "    cols_with_nan  # drop cols with over 90% nan, fillna the rest with median values\n",
    "\n",
    "    cols_to_drop = cols_with_nan[cols_with_nan['percentage'] > 90].index\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Fill numerical NaN with median\n",
    "    numerical_columns = df.select_dtypes(include='number').columns\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].median())\n",
    "    \n",
    "    # Fill categorical NaN with mode\n",
    "    categorical_columns = df.select_dtypes(exclude='number').columns\n",
    "    df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])\n",
    "\n",
    "    df = df.drop(columns=[\"clntnum\"])\n",
    "\n",
    "    categorical_columns = df.select_dtypes(include='object').columns\n",
    "    columns_to_convert = [col for col in categorical_columns if col not in nominal_data]\n",
    "    \n",
    "    # Convert categorical columns to numerical\n",
    "    df[columns_to_convert] = df[columns_to_convert].astype('category').apply(lambda x: x.cat.codes)\n",
    "    \n",
    "    model_trained = joblib.load(\"random_forest_model_fold_0.joblib\")\n",
    "    \n",
    "    res = model_trained.predict(df)\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T15:44:46.438372800Z",
     "start_time": "2024-01-29T15:44:39.987144700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0., 1.]), array([17987,     5], dtype=int64))"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "x = testing_hidden_data(test_df)\n",
    "np.unique(x, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
